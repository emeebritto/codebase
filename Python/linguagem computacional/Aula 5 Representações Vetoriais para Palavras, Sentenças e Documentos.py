# -*- coding: utf-8 -*-
"""lc_aula5_vectors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BuLyMlebp43-3KNn-puezjW5S1CGuSI5

# Aula 5: Representações Vetoriais para Palavras, Sentenças e Documentos

## DCC: Linguística Computacional

### Álgebra Linear

Ramo da matemática que estuda os espaços vetoriais e as operações em vetores. Em Python, a manipulação de vetores de acordo com a álgebra linear pode ser facilmente feita através da biblioteca numpy.
"""

import numpy as np

"""Vetores podem ser instanciados pelo método *array* e variantes como *zeros* e *ones*"""

vetor1 = np.array([1., 2., 1., 4.])
vetor2 = np.zeros(4)
vetor3 = np.ones(4)

print('Vetor 1:', vetor1)
print('Vetor 2:', vetor2)
print('Vetor 3:', vetor3)

"""Soma"""

vetor1 + vetor3

"""Subtração"""

vetor1 - vetor3

"""Multiplicação"""

vetor1 * vetor2

"""Divisão"""

vetor3 / vetor1

"""Multiplicação de Matrizes"""

np.dot(vetor1, vetor3)

"""## Similaridade por Cossenos

Normalmente em PLN, a distância entre dois vetores é calculada através da similaridade por cosseno.
"""

from sklearn.metrics.pairwise import cosine_similarity

po = np.array([[5, 10]])
mestre_tigresa = np.array([[7.5, 2.5]])

cosine_similarity(po, mestre_tigresa)[0][0]

v1 = np.array([[0, 0]])
v2 = np.array([[1, 1]])

cosine_similarity(v1, v2)[0][0]

henrique_v = np.array([13, 89, 4, 3])

julio_cesar = np.array([7, 62, 1, 2])

cosine_similarity([henrique_v], [julio_cesar])

henrique_v = np.array([13, 89, 4, 3])

noite_reis = np.array([0, 80, 58, 15])

cosine_similarity([henrique_v], [noite_reis])

"""## Representação *One-Hot*

Palavras e documentos são representados por vetores de dimensão do tamanho do vocabulário. Os vetores assumem valores binários (0 ou 1) 

Mais Informações: [Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)
"""

from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(handle_unknown='ignore')

X = [["no"], ["meio"], ["do"], ["caminho"], ["tinha"], ["uma"], ["pedra"]]

enc.fit(X)
vocab = list(enc.categories_[0])
vetores = enc.transform(X).toarray()

print('Vocabulário: ', vocab)
print()
print('Vetores')
vetores

"""Vetor One-Hot de *pedra*"""

vetores[vocab.index('pedra')]

"""## Matriz de Frequência Termo-Documento

Dado um vocabulário e um conjunto de documentos, as representações das palavras e dos documentos podem ser calculadas a partir da contagem de cada palavra em cada documento.

Mais informações: [Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
"""

from sklearn.feature_extraction.text import CountVectorizer

corpus = ['no meio do caminho tinha uma pedra',
 'tinha uma pedra no meio do caminho',
 'tinha uma pedra',
 'no meio do caminho tinha uma pedra']

vectorizer = CountVectorizer()

vetores = vectorizer.fit_transform(corpus)
vocab = vectorizer.get_feature_names()

print('Vocabulário')
print(vocab)
print()
print('Matrix')
print(vetores.toarray())

"""Acessando o vetor da palavra *meio*"""

vetores[:, vocab.index('meio')].transpose().toarray()

"""Acessando o vetor do verso 3: *tinha uma pedra*"""

vetores[2, :].toarray()

"""Customizando o contador com um tokenizador próprio"""

import nltk
nltk.download('punkt')

def tokenize(texto):
  return nltk.word_tokenize(texto, language='portuguese')

vectorizer = CountVectorizer(tokenizer=tokenize)

vetores = vectorizer.fit_transform(corpus)
vocab = vectorizer.get_feature_names()

print('Vocabulário')
print(vocab)
print()
print('Matrix')
print(vetores.toarray())

"""## Matriz Termo-Termo

Dado um vocabulário, a representação de uma palavra pode ser calculada a partir da contagem de sua co-ocorrência com cada palavra do vocabulário num determinado contexto (e.g. documento, sentença, etc.).
"""

corpus = ['no meio do caminho tinha uma pedra',
 'tinha uma pedra no meio do caminho',
 'tinha uma pedra',
 'no meio do caminho tinha uma pedra']

corpus_tok = [verso.split() for verso in corpus]

vocab = ["no", "meio", "do", "caminho", "tinha", "uma", "pedra"]
vetores = np.zeros((len(vocab), len(vocab)))

for verso in corpus_tok:
  for i, w1 in enumerate(vocab):
    for j, w2 in enumerate(vocab):
      if i != j:
        if w1 in verso and w2 in verso:
          vetores[i, j] += 1

print('Vocabulário')
print(vocab)
print()
print('Matrix')
print(vetores)

"""## Remoção de Palavras Vazias

Palavras vazias (e.g., artigos, preposições, etc.), que possuem alta frequência em todos os documentos, podem ser removidas da contagem para melhorar a distinção entre documentos
"""

import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')

from sklearn.feature_extraction.text import CountVectorizer

corpus = ['no meio do caminho tinha uma pedra',
 'tinha uma pedra no meio do caminho',
 'tinha uma pedra',
 'no meio do caminho tinha uma pedra']

vectorizer = CountVectorizer(stop_words=stopwords)

vetores = vectorizer.fit_transform(corpus)
vocab = vectorizer.get_feature_names()

print('Vocabulário')
print(vocab)
print()
print('Matrix')
print(vetores.toarray())

"""## TF-IDF

Mais informações: [Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)
"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

corpus = ['ainda que mal pergunte',
 'ainda que mal respondas',
 'ainda que mal te entenda',
 'ainda que mal repitas']

vectorizer = Pipeline([('count', CountVectorizer()),
                 ('tfid', TfidfTransformer())])

vetores = vectorizer.fit_transform(corpus)
vocab = vectorizer['count'].get_feature_names()

print('Vocabulário')
print(vocab)
print()
print('Matrix')
print(np.round(vetores.toarray(), 2))

"""Acessando o primeiro (*ainda que mal pergunte*) e terceiro (*ainda que mal te entenda*) versos e calculando a similaridade entre eles."""

verso1 = vetores[0, :]
verso3 = vetores[2, :]

cosine_similarity(verso1, verso3)[0][0]

"""## Word Embeddings

Baixando um modelo (leva alguns minutos)
"""

!wget http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s50.zip
!unzip download.php?file=embeddings%2Fword2vec%2Fcbow_s50.zip

"""Inicializando os word embeddings"""

from gensim.models import KeyedVectors
word2vec = KeyedVectors.load_word2vec_format('cbow_s50.txt')

"""Acessando o word embedding da palavra *menino*"""

word2vec['menino']

"""Palavras mais semelhantes ao verbo *estudar*"""

word2vec.most_similar('homem')

"""Similaridade por cosseno entre os word embeddings das palavras *menino* e *cachorro*"""

word2vec.similarity('menino', 'cachorro')

"""Inferência lógica para: *odiar* está para *odiando*, assim como *amar* está para..."""

word2vec.most_similar(positive=['amar', 'odiando'], negative=['odiar'])

"""## BERTimbau

Word embeddings sensíveis ao contexto
"""

!pip3 install transformers

import torch
from transformers import AutoTokenizer  # Or BertTokenizer
from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads
from transformers import AutoModel  # or BertModel, for BERT without pretraining heads

device = 'cpu'# torch.device('cuda' if torch.cuda.is_available() else 'cpu')

tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)
bert = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')
bert = bert.to(device)

texto = 'Eu vou ao banco pagar a conta hoje.'

# tokenizando o texto
input_ids = tokenizer.encode(texto, return_tensors='pt')
wordpieces = tokenizer.convert_ids_to_tokens(input_ids[0])

# salvando ponteiros para palavras
subwords_idx = [] # first subword of each word
for i, wordpiece in enumerate(wordpieces):
    if '##' not in wordpiece and i not in [0, len(wordpieces)-1]:
        subwords_idx.append(i)

# obtendo os vetores para as palavras
input_ids = input_ids.to(device)
with torch.no_grad():
  outs = bert(input_ids)
  vetores = outs[0][0, :]  

vetores[subwords_idx]