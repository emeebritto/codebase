# -*- coding: utf-8 -*-
"""lc_aula6_redesneurais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dk79ynrhR9jaLXCB0uASC7bQgVUo0pY4

# Aula 6: Aprendizagem de Máquina - Exemplos um-para-um

## DCC: Linguística Computacional

# Demonstração 1

Treinando um modelo para prever verbos e substantivos

### Importando as dependências
"""

import torch
import torch.nn as nn
from torch import optim

"""### Dados

Redes Neurais lidam com números. Portanto, as strings devem ser convertidas em índices, tanto os nomes quanto as classes.
"""

# dados rotulados
X = [
  { 'X': 'amar', 'y': 'verbo' },
  { 'X': 'André', 'y': 'substantivo' },
  { 'X': 'beber', 'y': 'verbo' },
  { 'X': 'Bárbara', 'y': 'substantivo' },
  { 'X': 'Danilo', 'y': 'substantivo' },
  { 'X': 'menino', 'y': 'verbo' },
  { 'X': 'mulher', 'y': 'verbo' },
  { 'X': 'sorrir', 'y': 'verbo' },
  { 'X': 'Vivian', 'y': 'substantivo' },
  { 'X': 'viver', 'y': 'verbo' },
]

# Mapping de classe para índice e índice para classe
c2id = { 'substantivo': 0, 'verbo': 1 }
id2c = { 0: 'substantivo', 1: 'verbo' }

# Mapping de nome para índice e índice para nome
x2id = { w['X']:i for i, w in enumerate(X) }
# criação de um símbolo OOV (out of vocabulary) para nomes fora do conjunto de treinamento
x2id['oov'] = len(x2id)
id2x = { i:w for (w, i) in x2id.items() }
x2id

"""### Modelo

Desenvolvido com PyTorch
"""

class Predictor(nn.Module):
  def __init__(self, inp_dim, n_classes, x2id):
    super(Predictor, self).__init__()
    self.x2id = x2id
    self.lookup = nn.Embedding(len(x2id), inp_dim)
    self.Wb = nn.Linear(inp_dim, n_classes)
    self.softmax = nn.LogSoftmax(1)

  def forward(self, X):
    idxs = []
    for x in X:
      try:
        idxs.append(self.x2id[x])
      except:
        idxs.append(self.x2id['oov'])
    idxs = torch.tensor(idxs)
    embeddings = self.lookup(idxs)
    z = self.Wb(embeddings)
    return self.softmax(z)

"""### Instanciando o modelo"""

inp_dim = 3
n_classes = len(c2id)
model = Predictor(inp_dim, n_classes, x2id)

"""### Instanciando função de erro, otimizador e dados em lotes"""

nepochs = 10
batch_size = 2
batch_status = 2
learning_rate = 0.1
criterion = nn.NLLLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

from torch.utils.data import DataLoader, Dataset

traindata = DataLoader(X, batch_size=batch_size, shuffle=True)

"""### Treinamento"""

for epoch in range(nepochs):
  losses = []
  for batch_idx, row in enumerate(traindata):
    X = row['X']
    y = torch.tensor([c2id[c] for c in row['y']])

    # Forward
    outputs = model(X)
    
    # Calculate loss
    loss = criterion(outputs, y)
    losses.append(float(loss))
    
    # Backpropagation
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    # Display
    if (batch_idx+1) % batch_status == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tTotal Loss: {:.6f}'.format(
                            epoch+1, batch_idx + 1, len(traindata),
                            100. * batch_idx / len(traindata), float(loss), 
                            round(sum(losses) / len(losses), 5)))

"""### Predição"""

val_X = ["jogar", "amar", "cantar", "Vivian", "jogar"]

outputs = model(val_X)
# print(outputs)
val_y = torch.argmax(outputs, 1).tolist()

[id2c[c] for c in val_y]

"""# Demonstração 2

Treinamento de um modelo para prever se um nome é masculino ou feminino.

## Importando dependências
"""

import torch
import torch.nn as nn
from torch import optim
import nltk
from nltk.corpus import names
nltk.download('names')

"""## Dados"""

from random import shuffle
masc = names.words('male.txt')
fem = names.words('female.txt')

data = [{ 'X': row, 'y': 'male' } for row in masc]
data += [{ 'X': row, 'y': 'female' } for row in fem]

name2id = {}
pos = 0
for name in data:
  if name['X'] not in name2id:
    name2id[name['X']] = pos
    pos += 1
name2id['oov'] = len(name2id.keys())
id2name = { i:name for (name, i) in name2id.items() }

c2id = { 'male': 0, 'female': 1 }
id2c = { 0: 'male', 1: 'female' }

"""## Separando Dados de Treino e Teste"""

shuffle(data)

size = int(len(data) * 0.2)
trainset = data[size:]
testset = data[:size]

"""## Modelo: Multi-layer Perceptron"""

class NamePredictor(nn.Module):
  def __init__(self, inp_dim, hdim, n_classes, x2id):
    super(NamePredictor, self).__init__()
    self.x2id = x2id
    self.lookup = nn.Embedding(len(x2id), inp_dim)
    self.layer1 = nn.Linear(inp_dim, hdim)
    self.activation1 = nn.Sigmoid()
    self.layer2 = nn.Linear(hdim, n_classes)
    self.softmax = nn.LogSoftmax(1)

  def forward(self, X):
    idxs = []
    for x in X:
      try:
        idxs.append(self.x2id[x])
      except:
        idxs.append(self.x2id['oov'])
    idxs = torch.tensor(idxs)
    embeddings = self.lookup(idxs)

    z1 = self.layer1(embeddings)
    a1 = self.activation1(z1)
    z2 = self.layer2(a1)
    return self.softmax(z2)

"""## Inicializando o modelo"""

inp_dim = 3
hdim = 10
n_classes = len(c2id)
model = NamePredictor(inp_dim, hdim, n_classes, name2id)

"""## Instanciando função de erro, otimizadores e dados em lotes"""

nepochs = 20
batch_size = 16
batch_status = 64
learning_rate = 0.01
criterion = nn.NLLLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

from torch.utils.data import DataLoader, Dataset

traindata = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testdata = DataLoader(testset, batch_size=batch_size, shuffle=True)

"""## Método para Avaliação"""

from sklearn.metrics import f1_score

def evaluate(testdata):
  y_real, y_pred = [], []
  for batch_idx, row in enumerate(testdata):
    outputs = model(row['X'])
    y_pred.extend(torch.argmax(outputs, 1).tolist())

    y_real.extend([c2id[c] for c in row['y']])

  print('F1-Score:', f1_score(y_real, y_pred))

"""## Treinamento"""

for epoch in range(nepochs):
  losses = []
  for batch_idx, row in enumerate(traindata):
    X = row['X']
    y = torch.tensor([c2id[c] for c in row['y']])

    # Forward
    outputs = model(X)
    
    # Calculate loss
    loss = criterion(outputs, y)
    losses.append(float(loss))
    
    # Backpropagation
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    # Display
    if (batch_idx+1) % batch_status == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tTotal Loss: {:.6f}'.format(
                            epoch+1, batch_idx + 1, len(traindata),
                            100. * batch_idx / len(traindata), float(loss), 
                            round(sum(losses) / len(losses), 5)))
    
  evaluate(testdata)

"""# Análise de Sentimentos

B2W Digital é uma empresa de comércio eletrônico criada no final de 2006 pela fusão entre Submarino, Shoptime, Americanas.com.

Neste exemplo, usaremos um [córpus](https://github.com/b2wdigital/b2w-reviews01) de revisão de produtos nestas plataformas. O objetivo é treinar um classificador para prever se uma revisão é **boa**, **neutra** ou **ruim**. 
"""

import torch
import torch.nn as nn
from torch import optim

"""## Baixando o córpus"""

!wget https://raw.githubusercontent.com/b2wdigital/b2w-reviews01/master/B2W-Reviews01.csv

"""## Carregando o córpus"""

import csv
from random import shuffle

with open('B2W-Reviews01.csv') as f:
  reader = csv.reader(f, delimiter=';', quotechar='\"')
  corpus = list(reader)

  header, corpus = corpus[0], corpus[1:]

shuffle(corpus)

"""## Separando o córpus em conjuntos de treinamento e teste"""

size = int(len(corpus) * 0.2)
treino = corpus[size:]
teste = corpus[:size]

len(treino), len(teste)

"""## Extraindo as features

Como features, será utilizado informações do produto, como nome, marca e categoria; assim como informações da revisão do produto, como título e conteúdo.

A partir destas informações, o objetivo é prever se a revisão do produto é **boa**, **neutra** ou **ruim**.
"""

def get_features(info):
  product_id = info[2]
  product_name = info[3]
  product_brand = info[4]
  site_category_lv1 = info[5]
  site_category_lv2 = info[6]
  review_title = info[7]
  overall_rating = info[8]
  recommend_to_a_friend = info[9]
  review_text = info[10]
  reviewer_birth_year = info[11]
  reviewer_gender = info[12]
  reviewer_state = info[13]

  x = ' '.join([product_name, 
                product_brand, 
                site_category_lv1, 
                site_category_lv2, 
                review_title, 
                review_text])
  y = 2 if overall_rating in ['4', '5'] else 0 if overall_rating in ['1', '2'] else 1
  return { 'X': x, 'y': y }

"""## Representação Vetorial TF-IDF

Assim como aprendemos na aula anterior, o texto de entrada com as informações do produto e da revisão será codificado num vetor TF-IDF. 

Uma vez codificado, o vetor será passado para uma Regressão Logística que fará a classificação.
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

def tokenize(texto):
  return nltk.word_tokenize(texto)

pipe = Pipeline([
  ('count', CountVectorizer(tokenizer=tokenize, stop_words=stopwords, min_df=5)),
  ('tfidf', TfidfTransformer()),
])

"""Treinando o modelo TF-IDF"""

feat_treino = [get_features(w) for w in treino]
treino_X = [w['X'] for w in feat_treino]
treino_y = [w['y'] for w in feat_treino]

pipe.fit(treino_X, treino_y)

nvocab = pipe.transform(["Test"]).shape[1]
nvocab

"""## Regressão Logística"""

class SentimentPredictor(nn.Module):
  def __init__(self, inp_dim, hdim, n_classes, tfidfer):
    super(SentimentPredictor, self).__init__()
    self.tfidfer = tfidfer
    self.layer1 = nn.Linear(inp_dim, hdim)
    self.activation1 = nn.Sigmoid()
    self.layer2 = nn.Linear(hdim, n_classes)
    self.softmax = nn.LogSoftmax(1)

  def forward(self, X):
    tfidf = self.tfidfer.transform(X).toarray()
    tfidf = torch.tensor(tfidf).float()

    z1 = self.layer1(tfidf)
    a1 = self.activation1(z1)
    z2 = self.layer2(a1)
    return self.softmax(z2)

"""## Inicializando o Modelo"""

inp_dim = nvocab
hdim = 1024
n_classes = len(set(treino_y))
model = SentimentPredictor(inp_dim, hdim, n_classes, pipe)

"""## Instanciando função de erro, otimizador e dados de treinamento e teste em lotes"""

nepochs = 5
batch_size = 256
batch_status = 64
learning_rate = 0.01
criterion = nn.NLLLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

from torch.utils.data import DataLoader, Dataset

traindata = DataLoader([get_features(x) for x in treino], batch_size=batch_size, shuffle=True)
testdata = DataLoader([get_features(x) for x in teste], batch_size=batch_size, shuffle=True)

"""## Avaliação"""

from sklearn.metrics import f1_score

def evaluate(testdata):
  y_real, y_pred = [], []
  for batch_idx, row in enumerate(testdata):
    outputs = model(row['X'])
    y_pred.extend(torch.argmax(outputs, 1).tolist())

    y_real.extend(row['y'])

  print('F1-Score:', f1_score(y_real, y_pred, average='weighted'))

"""## Treinamento"""

for epoch in range(nepochs):
  losses = []
  for batch_idx, row in enumerate(traindata):
    X = row['X']
    y = row['y']

    # Forward
    outputs = model(X)
    
    # Calculate loss
    loss = criterion(outputs, y)
    losses.append(float(loss))
    
    # Backpropagation
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    # Display
    if (batch_idx+1) % batch_status == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tTotal Loss: {:.6f}'.format(
                            epoch+1, batch_idx + 1, len(traindata),
                            100. * batch_idx / len(traindata), float(loss), 
                            round(sum(losses) / len(losses), 5)))
    
  evaluate(testdata)

