# -*- coding: utf-8 -*-
"""lc_aula3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SFzI80FR6FjrMkEUGSvL7Y48PH2RYSbq

# Aula 3 - Normalização de Texto
## DCC: Linguística Computacional

## Tokenizadores

Importando a biblioteca de expressões regulares do Python
"""

import re

"""### Tokenizador por Espaço

Desenvolvido com base no principal delimitador para uma grande parcela das línguas naturais humanas: o espaço
"""

texto = "No meio do caminho tinha uma pedra."

texto.split()

"""### Tokenizador baseado numa expressão regular

Segmenta as palavras de um texto com base em delimitadores como espaço, pontuações e início/fim de uma sequência (\b)
"""

texto = "No meio do caminho tinha uma pedra."

re.sub(r"(\b)", r" \1", texto).split()

"""### Tokenizador baseado em Regras

1. Buscar todas as ocorrências de valores numéricos e financeiros (R$1,00; $46; etc.)

2. Buscar todas as ocorrências de sequências de 1 ou mais caracteres

3. Buscar todas as ocorrências de sequências sem espaço

"""

texto = "Eu paguei R$456,00 pelo setup. O que acha?"
regex = r"R?\$?[\d\.\,]+|\w+|\S+"
re.findall(regex, texto)

"""### Tokenizador baseado em Regras do NLTK"""

import nltk
# nltk.download('punkt')

versos = """O menino jogou bola ontem às 16:00."""

nltk.word_tokenize(versos, language='english')

text = """Hello everyone!!!"""

nltk.word_tokenize(text, language='english')

"""### Byte-Pair Encoding (BPE)

[Fonte](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html)

Baixando as dependências
"""

!pip3 install transformers

"""Baixando córpus do Wikipedia para treinar o tokenizador

[Fonte](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)
"""

!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
!unzip wikitext-103-raw-v1.zip

"""Inicializando o tokenizador"""

from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

"""Inicializando o Módulo de Treinamento

Define-se um vocabulário desejado com 30000 símbolos
"""

from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], 
                     vocab_size=30000, 
                     min_frequency=0,
                     continuing_subword_prefix="##")

"""Definindo pré-tokenizador por espaço"""

from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()

"""Treinando o tokenizador"""

files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

"""Salvando o Tokenizador"""

tokenizer.save("tokenizer-wiki.json")

"""Carregando o tokenizador"""

tokenizer = Tokenizer.from_file("tokenizer-wiki.json")

"""Tokenizando textos"""

texto = "I don't go out tonight."
output = tokenizer.encode(texto)
output.tokens

tokenizer.decode(output.ids)

"""### Byte-level BPE

[Fonte](https://huggingface.co/blog/how-to-train)

Inicializando o tokenizador e o córpus de treinamento (Wikipedia)
"""

from tokenizers import ByteLevelBPETokenizer

files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]

tokenizer = ByteLevelBPETokenizer()

"""Treinando o tokenizador"""

tokenizer.train(files=files, vocab_size=52_000, min_frequency=2, special_tokens=[
    "<s>",
    "<pad>",
    "</s>",
    "<unk>",
    "<mask>",
])

"""Salvando o tokenizador"""

tokenizer.save("tokenizer-wiki.json")

"""Tokenizando um texto"""

output = tokenizer.encode("Eu estou na aula de Línguística Computacional.")
output.tokens

"""### WordPiece

Inicializando o Tokenizador
"""

from tokenizers import Tokenizer
from tokenizers.models import WordPiece

files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]

tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

"""Inicializando o módulo de treinamento"""

from tokenizers.trainers import WordPieceTrainer

trainer = WordPieceTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], 
                     vocab_size=30000, 
                     min_frequency=0,
                     continuing_subword_prefix="##")

"""Inicializando pré-tokenizador"""

from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()

"""Treinando o tokenizador"""

files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

"""Tokenizando o texto"""

output = tokenizer.encode("Eu estou na aula.")
output.tokens

"""### Unigram

Semelhante aos outros modelos. Pode precisar de GPU
"""

from tokenizers import Tokenizer
from tokenizers.models import Unigram
from tokenizers.trainers import UnigramTrainer

files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]

# inicializando o tokenizador
tokenizer = Tokenizer(Unigram())
# inicilizando o módulo de treinamento
trainer = UnigramTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], 
                     vocab_size=30000, 
                     min_frequency=0,
                     continuing_subword_prefix="##")
# treinando o tokenizador
files = [f"wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

output = tokenizer.encode("I will play games tonight.")
output.tokens

"""## Capitalização

Processo de colocar os tokens em letra minúscula para normalização do texto.
"""

import nltk
# nltk.download('punkt')

versos = """No meio do caminho tinha uma pedra
Tinha uma pedra no meio do caminho""".lower()

nltk.word_tokenize(versos, language='portuguese')

"""### Tokenização de Sentenças"""

import nltk
# nltk.download('punkt')

texto = 'Eu estou na aula de Linguística Computacional. Os estudantes são muito bons.'

nltk.sent_tokenize(texto, language='portuguese')

"""## Lematização

Instalando o Spacy
"""

!pip3 install -U pip setuptools wheel
!pip3 install -U spacy[cuda102]==3
!python3 -m spacy download pt_core_news_lg

"""Inicializando o modelo para Português"""

import spacy

spacy.prefer_gpu()
nlp = spacy.load("pt_core_news_lg")

doc = nlp("O passado é só uma história que nos contamos.")

for token in doc:
  print(token, token.lemma_)

"""## Radicalização"""

import nltk
# nltk.download('rslp')

raiz = nltk.stem.RSLPStemmer()

tokens = nltk.word_tokenize('A comida estava gostosa', language='portuguese')
[raiz.stem(token) for token in tokens]

