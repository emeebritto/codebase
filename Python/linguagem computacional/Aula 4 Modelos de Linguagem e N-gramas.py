# -*- coding: utf-8 -*-
"""lc_aula4_lm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DrZF3OFC4Y-YVTRHdtbNNH6ADN_86TPb

# Aula 4: Modelos de Linguagem e N-gramas

## DCC: Linguística Computacional

Baixando a versão 3.5 do NLTK
"""

!pip3 install nltk==3.5

"""### Importando Dependências"""

import nltk
nltk.download('punkt')

"""### Passo 1: Carregando o Córpus"""

texto = """No meio do caminho tinha uma pedra
Tinha uma pedra no meio do caminho
Tinha uma pedra
No meio do caminho tinha uma pedra"""

texto = texto.lower().split('\n')
texto

"""### Passo 2: Tokenizando as Sentenças do Córpus

(Lembrem-se da Aula 2: Segmentação e Padronização de Textos)
"""

texto_tok = []
for verso in texto:
  tokens = nltk.word_tokenize(verso, language='portuguese')
  texto_tok.append(tokens)

texto_tok

"""## Pré-processando as Sentenças

### Passo 3: Inserindo Marcadores de Início e Fim de Sentença
Suponha que queiramos definir um modelo de linguagem com bigramas, ou seja, calcular as chances de uma palavra com base na anterior (e.g., $P(pedra | uma)$, temos que marcar o início e fim da sentença para poder prever as changes da primeira palavra (e.g., $P(no | \langle s \rangle)$) e o fim da sentença ((e.g., $P(\langle/ s \rangle)$ | pedra)). Este processo é conhecido como *padding*.

Podemos fazer o *padding* de uma sentença utilizando o método **nltk.lm.preprocessing.pad_both_ends**:
"""

from nltk.lm.preprocessing import pad_both_ends

ngramas = 2 # definindo o número de n-gramas (no caso, 2 -> bigramas)

texto_tok_pad = []
for verso in texto_tok:
  padded = pad_both_ends(verso, n=ngramas)
  texto_tok_pad.append(list(padded))

texto_tok_pad

"""### Passo 4: Calculando os N-Gramas

Uma vez que as sentenças do nosso córpus foram pré-processadas, podemos calcular os n-gramas (neste caso, os bigramas), utilizando o método **nltk.ngrams**:
"""

ngramas = 2

bigramas_pad = []
for verso in texto_tok_pad:
  bigramas = nltk.ngrams(verso, ngramas)
  bigramas_pad.append(list(bigramas))

bigramas_pad

"""Contudo, para deixar nosso modelo de linguagem mais robusto, vamos calcular os **unigramas** além dos **bigramas** utilizando o comando **nltk.util.everygrams**:"""

from nltk.util import everygrams
ngramas = 2

ngramas_pad = []
for verso in texto_tok_pad:
  bigramas = everygrams(verso, max_len=ngramas)
  ngramas_pad.append(list(bigramas))

ngramas_pad

"""### Passo 5: Colocando todos os tokens do córpus numa única lista
**nltk.lm.preprocessing.flatten**:

Este método converte junta os elementos de sublistas em uma única lista. Por exemplo:

```python
>>> lista = [[1, 2], [3, 4]]
>>> flatten(lista)
[1, 2, 3, 4]
```

Como pode ser visto abaixo, nós o utilizamos para juntar todas os tokens das sentenças de nosso corpus numa única lista.
"""

from nltk.lm.preprocessing import flatten

tokens = list(flatten(texto_tok_pad)) # juntando as palavras do nosso córpus

tokens

"""### Passo 6: Definindo o Vocabulário

**nltk.lm.Vocabulary**

Utilizado para definir o vocabulário do nosso córpus. Recebe dois parâmetros como entrada: uma lista com todos os tokens do nosso córpus e a variável *unk_cutoff*, a qual passa a considerar palavras abaixo de um limiar de frequência como palavras fora do vocabuário.
"""

from nltk.lm import Vocabulary

vocab = Vocabulary(tokens, unk_cutoff=1) # definindo o vocabulário do nosso córpus

"""Obtendo as frequências das palavras do córpus com o comando *counts*"""

vocab.counts

"""procurando uma palavra no vocabulário. Caso não encontrada, o token de palavra fora do vocabulário será retornada (\<UNK>)"""

vocab.lookup("tinha"), vocab.lookup("homem")

"""## Simplificando o Pré-processamento

Agora que você sabe cada passo do pré-processamento (inserir marcadores de início e fim de sentença, calcular os n-gramas, juntar todos os tokens do corpus numa lista e definir o vocabulário), este processo pode ser simplificado pela funcionalidade **nltk.lm.preprocessing.padded_everygram_pipeline**:
"""

from nltk.lm.preprocessing import padded_everygram_pipeline

ngramas = 2

ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)

ngramas_pad = [list(w) for w in ngramas_pad]
ngramas_pad

"""## Passo 7: Treinando um modelo de linguagem

Um modelo de linguagem pode ser treinado utilizando a funcionalidade **nltk.lm.MLE**
"""

from nltk.lm.preprocessing import padded_everygram_pipeline, flatten
from nltk.lm import MLE

ngramas = 2
ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)
lm = MLE(ngramas)
lm.fit(ngramas_pad, vocab)

"""Dado o token **\<s>**, gerando um texto de 4 tokens com o modelo de linguagem treinado."""

lm.generate(4, text_seed=["<s>"])

"""Probabilidade da palavra *no*:"""

print("Probabilidade comum: ", lm.score("no"))
print("Probabilidade logarítmica: ", lm.logscore("no"))

"""Probabilidade da palavra *tinha* dado a palavra *caminho*:"""

print("Probabilidade comum: ", lm.score("tinha", context=["caminho"]))
print("Probabilidade logarítmica: ", lm.logscore("tinha", context=["caminho"]))

"""## Avaliação Perplexidade"""

teste = """Tinha uma pedra
No meio do caminho
Tinha uma pedra"""

# pré-processamento
teste = teste.lower().split('\n')
teste_tok = []
for verso in teste:
  tokens = nltk.word_tokenize(verso, language='portuguese')
  teste_tok.append(tokens)

ngramas = 1
teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)
teste_ngramas = flatten([list(w) for w in teste_ngramas])
print("Perplexidade do Unigrama: ", lm.perplexity(teste_ngramas))

ngramas = 2
teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)
teste_ngramas = flatten([list(w) for w in teste_ngramas])
print("Perplexidade do Bigrama: ", lm.perplexity(teste_ngramas))

"""## Add-1 Smoothing"""

from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.lm import Laplace

ngramas = 2
ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)
lm = Laplace(ngramas)
lm.fit(ngramas_pad, vocab)

round(lm.score("tinha", context=["caminho"]), 2)

"""## Add-k Smoothing"""

from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.lm import Lidstone

ngramas = 2
k=0.1
ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)
lm = Lidstone(order=ngramas, gamma=k)
lm.fit(ngramas_pad, vocab)

round(lm.score("tinha", context=["caminho"]), 2)

